{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train size: 8196\n",
      "test size: 2049\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# load data\n",
    "train = pd.read_csv('../data/processed/train.csv')\n",
    "test = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "print('train size:', len(train))\n",
    "print('test size:', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean: 4.2269399707174236\n"
     ]
    }
   ],
   "source": [
    "# global mean predictor\n",
    "global_mean = train['rating'].mean()\n",
    "print('Global Mean:', global_mean)\n",
    "\n",
    "# user mean\n",
    "ratingsPerUser = defaultdict(list)\n",
    "for _, row in train.iterrows():\n",
    "    ratingsPerUser[row['user_id']].append(row['rating'])\n",
    "\n",
    "# compute mean for each user\n",
    "userMean = {}\n",
    "for u in ratingsPerUser:\n",
    "    userMean[u] = np.mean(ratingsPerUser[u])\n",
    "    \n",
    "# item mean\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for _, row in train.iterrows():\n",
    "    ratingsPerItem[row['item_id']].append(row['rating'])\n",
    "\n",
    "# compute mean for each item\n",
    "itemMean = {}\n",
    "for i in ratingsPerItem:\n",
    "    itemMean[i] = np.mean(ratingsPerItem[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Model Structures\n",
    "ratingsPerUser_bias = defaultdict(list)\n",
    "ratingsPerItem_bias = defaultdict(list)\n",
    "\n",
    "for _, row in train.iterrows():\n",
    "    u = row['user_id']\n",
    "    i = row['item_id']\n",
    "    r = row['rating']\n",
    "    ratingsPerUser_bias[u].append((i,r))\n",
    "    ratingsPerItem_bias[i].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias Model Training\n",
    "alpha = global_mean\n",
    "betaU = defaultdict(float)\n",
    "betaI = defaultdict(float)\n",
    "lamb = 5\n",
    "max_iter = 20\n",
    "\n",
    "def update_alpha(train, betaU, betaI):\n",
    "    numer = 0\n",
    "    for idx, row in train.iterrows():\n",
    "        u = row['user_id']\n",
    "        i = row['item_id']\n",
    "        r = row['rating']\n",
    "        numer += r - (betaU.get(u,0) + betaI.get(i,0))\n",
    "    return numer / len(train)\n",
    "\n",
    "def update_betaU(ratingsPerUser, alpha, betaI, lamb):\n",
    "    newBetaU = {}\n",
    "    for u, items in ratingsPerUser.items():\n",
    "        numer = 0\n",
    "        for (i, r) in items:\n",
    "            numer += r - (alpha + betaI.get(i,0))\n",
    "        newBetaU[u] = numer / (lamb + len(items))\n",
    "    return newBetaU\n",
    "\n",
    "def update_betaI(ratingsPerItem, alpha, betaU, lamb):\n",
    "    newBetaI = {}\n",
    "    for i, users in ratingsPerItem.items():\n",
    "        numer = 0\n",
    "        for (u, r) in users:\n",
    "            numer += r - (alpha + betaU.get(u,0))\n",
    "        newBetaI[i] = numer / (lamb + len(users))\n",
    "    return newBetaI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: alpha=4.23, MSE=0.73\n",
      "Iter 2: alpha=4.21, MSE=0.74\n",
      "Iter 3: alpha=4.20, MSE=0.74\n",
      "Iter 4: alpha=4.19, MSE=0.74\n",
      "Iter 5: alpha=4.19, MSE=0.74\n",
      "Iter 6: alpha=4.19, MSE=0.74\n",
      "Iter 7: alpha=4.19, MSE=0.74\n",
      "Iter 8: alpha=4.19, MSE=0.74\n",
      "Iter 9: alpha=4.19, MSE=0.74\n",
      "Iter 10: alpha=4.19, MSE=0.74\n",
      "Iter 11: alpha=4.19, MSE=0.74\n",
      "Iter 12: alpha=4.19, MSE=0.74\n",
      "Iter 13: alpha=4.19, MSE=0.74\n",
      "Iter 14: alpha=4.19, MSE=0.74\n",
      "Iter 15: alpha=4.19, MSE=0.74\n",
      "Iter 16: alpha=4.19, MSE=0.74\n",
      "Iter 17: alpha=4.19, MSE=0.74\n",
      "Iter 18: alpha=4.19, MSE=0.74\n",
      "Iter 19: alpha=4.19, MSE=0.74\n",
      "Iter 20: alpha=4.19, MSE=0.74\n"
     ]
    }
   ],
   "source": [
    "for t in range(max_iter):\n",
    "    alpha = update_alpha(train, betaU, betaI)\n",
    "    betaU = update_betaU(ratingsPerUser_bias, alpha, betaI, lamb)\n",
    "    betaI = update_betaI(ratingsPerItem_bias, alpha, betaU, lamb)\n",
    "\n",
    "    # compute train MSE\n",
    "    mse = 0\n",
    "    for _, row in train.iterrows():\n",
    "        u = row['user_id']\n",
    "        i = row['item_id']\n",
    "        r = row['rating']\n",
    "        pred = alpha + betaU.get(u,0) + betaI.get(i,0)\n",
    "        pred = max(1, min(5, pred))\n",
    "        mse += (r - pred)**2\n",
    "    mse /= len(train)\n",
    "\n",
    "    print(f\"Iter {t+1}: alpha={alpha:.2f}, MSE={mse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Mean Test MSE: 1.5591178641582832\n",
      "User Mean Test MSE: 1.9124749905308376\n",
      "Item Mean Test MSE: 1.9867923491024322\n"
     ]
    }
   ],
   "source": [
    "### Baseline Test MSEs\n",
    "# Global Mean Test MSE\n",
    "gm_preds = [global_mean] * len(test)\n",
    "mse_gm = mean_squared_error(test['rating'], gm_preds)\n",
    "print(\"Global Mean Test MSE:\", mse_gm)\n",
    "\n",
    "# User Mean Test MSE\n",
    "user_preds = []\n",
    "for _, row in test.iterrows():\n",
    "    u = row['user_id']\n",
    "    if u in userMean:\n",
    "        pred = userMean[u]\n",
    "    else:\n",
    "        pred = global_mean\n",
    "    user_preds.append(pred)\n",
    "\n",
    "mse_user = mean_squared_error(test['rating'], user_preds)\n",
    "print(\"User Mean Test MSE:\", mse_user)\n",
    "\n",
    "# Item Mean Test MSE\n",
    "item_preds = []\n",
    "for _, row in test.iterrows():\n",
    "    i = row['item_id']\n",
    "    if i in itemMean:\n",
    "        pred = itemMean[i]\n",
    "    else:\n",
    "        pred = global_mean\n",
    "    item_preds.append(pred)\n",
    "\n",
    "mse_item = mean_squared_error(test['rating'], item_preds)\n",
    "print(\"Item Mean Test MSE:\", mse_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias model Test MSE: 1.437052575369529\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Bias Model on Test\n",
    "test_preds = []\n",
    "\n",
    "for _, row in test.iterrows():\n",
    "    u = row['user_id']\n",
    "    i = row['item_id']\n",
    "    pred = alpha + betaU.get(u,0) + betaI.get(i,0)\n",
    "    pred = max(1.0, min(5.0, pred))\n",
    "    test_preds.append(pred)\n",
    "\n",
    "mse_test = mean_squared_error(test['rating'], test_preds)\n",
    "print(\"Bias model Test MSE:\", mse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved baseline table to: ../results/tables/baseline_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Global Mean</td>\n",
       "      <td>1.559118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>User Mean</td>\n",
       "      <td>1.912475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Item Mean</td>\n",
       "      <td>1.986792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bias Model</td>\n",
       "      <td>1.437053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Model  Test MSE\n",
       "0  Global Mean  1.559118\n",
       "1    User Mean  1.912475\n",
       "2    Item Mean  1.986792\n",
       "3   Bias Model  1.437053"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {\n",
    "    \"Model\": [\"Global Mean\", \"User Mean\", \"Item Mean\", \"Bias Model\"],\n",
    "    \"Test MSE\": [mse_gm, mse_user, mse_item, mse_test]\n",
    "}\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "# save\n",
    "output_path = \"../results/tables/baseline_results.csv\"\n",
    "df_results.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Saved baseline table to:\", output_path)\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
